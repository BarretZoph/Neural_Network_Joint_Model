{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NNJM\n",
    "#TODO\n",
    "# Check to be sure slices are being done correctly\n",
    "# Convolution instead of first dense hidden layer\n",
    "# Put in character CNN\n",
    "# Put in bi-dir LSTm\n",
    "# Add in different loss function\n",
    "# Add in argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import statements\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters for the model\n",
    "params = {}\n",
    "params['minibatch'] = 50\n",
    "params['emb-size'] = 200\n",
    "params['hiddenstate-size'] = 512\n",
    "params['source-vocab'] = 500\n",
    "params['target-vocab'] = 600\n",
    "params['datatype'] = tf.float32\n",
    "params['init-method'] = 'uniform'\n",
    "params['init-range'] = 0.01\n",
    "params['source-window'] = 11\n",
    "params['target-window'] = 4\n",
    "params['use-char'] = False\n",
    "params['seed'] = 1\n",
    "params['dropout-rate'] = 0.0\n",
    "params['loss'] = 'MLE' #('MLE','NCE','IS')\n",
    "params['learning-rate'] = 0.5\n",
    "params['val-check-rate'] = 0.5 #every fraction of the training set get val perplexity\n",
    "\n",
    "#Character model params\n",
    "char_params = {}\n",
    "char_params['num-highway-layers'] = 4\n",
    "char_params['char-emb-size'] = 25\n",
    "char_params['filter-width'] = 7\n",
    "char_params['longest-word'] = 30\n",
    "\n",
    "\n",
    "params['char-params'] = char_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert params['val-check-rate']>0\n",
    "assert params['val-check-rate']<=1\n",
    "assert params['dropout-rate']>=0\n",
    "assert params['dropout-rate']<=1\n",
    "assert params['init-range']>0\n",
    "assert params['minibatch']>0\n",
    "assert params['learning-rate']>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Interactive session\n",
    "np.random.seed(seed=params['seed'])\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Process the data, get the source-vocab and target-vocab variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Place holders for input and output data, first index is the minibatch size\n",
    "\n",
    "#For the input the second dimension will be passed a\n",
    "#     vector of size minibatch x (params['source-window']+params['target-window'])\n",
    "with tf.device('/cpu:0'):\n",
    "    input_indices = tf.placeholder(tf.int64, shape=[None, params['source-window']+params['target-window']])\n",
    "\n",
    "#The output will be given a minibatch vector of correct indicies\n",
    "with tf.device('/cpu:0'):\n",
    "    correct_output = tf.placeholder(tf.int64,shape=[None])\n",
    "\n",
    "#For passing in the learning rate and dropout rate\n",
    "with tf.device('/cpu:0'):\n",
    "    learning_rate = tf.placeholder(params['datatype'],shape=[])\n",
    "    dropout_rate = tf.placeholder(params['datatype'],shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def param_init(params,shape=None,name=None,datatype=None):\n",
    "    if datatype == None:\n",
    "        datatype = params['datatype']\n",
    "    assert shape != None,\"Error shape cannot be None in param_init\"\n",
    "    if params['init-method'] == 'uniform':\n",
    "        return tf.random_uniform(shape, minval=-1*params['init-range'],maxval=params['init-range'],\\\n",
    "                    dtype=datatype, seed=params['seed'], name=name)\n",
    "    else:\n",
    "        print \"ERROR this init-method has not been created yet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parameters for the source and target embeddings\n",
    "with tf.device('/cpu:0'):\n",
    "    source_emb_matrix = tf.Variable(param_init(params,shape=[params['source-vocab'],params['emb-size']],name='src-emb'))\n",
    "    target_emb_matrix = tf.Variable(param_init(params,shape=[params['target-vocab'],params['emb-size']],name='tgt-emb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Do the one-hot emebedding lookups\n",
    "src_embed = tf.nn.embedding_lookup(source_emb_matrix, tf.slice(input_indices,[0,0],[-1,params['source-window']]))\n",
    "tgt_embed = tf.nn.embedding_lookup(target_emb_matrix, tf.slice(input_indices,[0,params['source-window']],\\\n",
    "                                                                [-1,params['target-window']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now reshape to be able to feed through non-linearity\n",
    "concat_embed = tf.concat(1, [src_embed, tgt_embed])\n",
    "concat_embed = tf.reshape(concat_embed,[-1,params['emb-size']*(params['source-window']+params['target-window'])])\n",
    "concat_embed = tf.nn.dropout(concat_embed, 1-params['dropout-rate']) #Pass in the keep prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First Layer\n",
    "with tf.device('/cpu:0'):\n",
    "    layer_1_weights = tf.Variable(param_init(params,\\\n",
    "        shape=[params['emb-size']*(params['source-window']+params['target-window']),params['hiddenstate-size']],name='lyr-1'))\n",
    "    layer_1_bias = tf.Variable(param_init(params,shape=[params['hiddenstate-size']],name='lyr-1-bias'))\n",
    "layer_1_output = tf.nn.relu(tf.matmul(concat_embed,layer_1_weights)+layer_1_bias)\n",
    "layer_1_output = tf.nn.dropout(layer_1_output, 1-params['dropout-rate']) #Pass in the keep prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Second layer\n",
    "with tf.device('/cpu:0'):\n",
    "    layer_2_weights = tf.Variable(param_init(params,shape=[params['hiddenstate-size'],params['hiddenstate-size']],\\\n",
    "                            name='lyr-2'))\n",
    "    layer_2_bias = tf.Variable(param_init(params,shape=[params['hiddenstate-size']],name='lyr-2-bias'))\n",
    "layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,layer_2_weights)+layer_2_bias)\n",
    "layer_2_output = tf.nn.dropout(layer_2_output, 1-params['dropout-rate']) #Pass in the keep prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Softmax layer\n",
    "with tf.device('/cpu:0'):\n",
    "    softmax_weights = tf.Variable(param_init(params,shape=[params['hiddenstate-size'],params['target-vocab']],\\\n",
    "                                  name='softmax-weights')) \n",
    "    softmax_bias = tf.Variable(param_init(params,shape=[params['target-vocab']],name='softmax-bias'))\n",
    "unscaled_final_output = tf.matmul(layer_2_output, softmax_weights) + softmax_bias\n",
    "loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(unscaled_final_output,correct_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Additional variables for tracking loss\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
