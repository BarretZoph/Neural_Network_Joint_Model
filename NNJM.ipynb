{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NNJM\n",
    "#TODO\n",
    "# Check to be sure slices are being done correctly\n",
    "# Convolution instead of first dense hidden layer\n",
    "# Put in character CNN\n",
    "# Put in bi-dir LSTm\n",
    "# Add in different loss function\n",
    "# Add in argparse\n",
    "# Halve the learning rate is perp increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import statements\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import codecs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters for the model\n",
    "params = {}\n",
    "params['minibatch'] = 32\n",
    "params['emb-size'] = 256\n",
    "params['hiddenstate-size'] = 256\n",
    "params['source-vocab'] = None\n",
    "params['target-vocab'] = None\n",
    "params['datatype'] = tf.float32\n",
    "params['init-method'] = 'uniform'\n",
    "params['init-range'] = 0.01\n",
    "params['source-window'] = 11\n",
    "params['target-window'] = 4\n",
    "params['use-char'] = False\n",
    "params['seed'] = 1\n",
    "params['dropout-rate'] = 0.0 #probability of dropping a node\n",
    "params['loss'] = 'MLE' #('MLE','NCE','IS')\n",
    "params['learning-rate'] = 0.001\n",
    "params['val-check-rate'] = 0.5 #every fraction of the training set get val perplexity\n",
    "params['epochs'] = 1000 #how many epochs to train for\n",
    "params['minibatches-per-epoch'] = None #Filled in by the data preprep\n",
    "params['source-train-file'] = 'train.source.lc'\n",
    "params['target-train-file'] = 'train.target.lc'\n",
    "params['mapping-file-name'] = 'mapping.nn'\n",
    "params['count-cutoff'] = 3\n",
    "params['training-data-file-name'] = 'training.data.11+4.small'\n",
    "params['val-data-file-name'] = 'training.data.11+4.small'#'validation.data.11+4'\n",
    "\n",
    "#Character model params\n",
    "char_params = {}\n",
    "char_params['num-highway-layers'] = 4\n",
    "char_params['char-emb-size'] = 25\n",
    "char_params['filter-width'] = 7\n",
    "char_params['longest-word'] = 30\n",
    "\n",
    "\n",
    "params['char-params'] = char_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert params['val-check-rate']>0\n",
    "assert params['val-check-rate']<=1\n",
    "assert params['dropout-rate']>=0\n",
    "assert params['dropout-rate']<=1\n",
    "assert params['init-range']>0\n",
    "assert params['minibatch']>0\n",
    "assert params['learning-rate']>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Interactive session\n",
    "np.random.seed(seed=params['seed'])\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Process the data, get the source-vocab and target-vocab variables\n",
    "import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source file name used for creating mapping file: train.source.lc\n",
      "target file name used for creating mapping file: train.target.lc\n",
      "source vocab size: 46166\n",
      "target vocab size: 22701\n"
     ]
    }
   ],
   "source": [
    "#create the mapping file\n",
    "data_loader.create_word_mapping_file(params['source-train-file'],params['target-train-file'],\\\n",
    "            params['mapping-file-name'],params['count-cutoff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source mapping size: 46166\n",
      "Target mapping size: 22701\n",
      "src_window: 11\n",
      "tgt_window: 4\n",
      "minibatch_size: 32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-41f9a44c3d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Now load in the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mapping-file-name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training-data-file-name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val-data-file-name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source-window'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target-window'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'minibatch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val-check-rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'minibatches-per-epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatches_per_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source-vocab'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target-vocab'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/barretzoph/Desktop/TensorFlow/NNJM/data_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mapping_file, data_file_name, val_file_name, src_window, tgt_window, minibatch_size, val_check_rate)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"tgt_window:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"minibatch_size:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;31m#now set the checkpoints for getting val score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_checkpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/barretzoph/Desktop/TensorFlow/NNJM/data_loader.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, data_file_name, val_file_name)\u001b[0m\n\u001b[1;32m     97\u001b[0m                                         \u001b[0msrc_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                                 \u001b[0;32massert\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'<unk'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Error the dataset has already been unked\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                                         \u001b[0mtgt_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Now load in the data\n",
    "data_factory = data_loader.minibatcher(params['mapping-file-name'],params['training-data-file-name'],\\\n",
    "                params['val-data-file-name'],params['source-window'],params['target-window'],\\\n",
    "                params['minibatch'],params['val-check-rate'])\n",
    "params['minibatches-per-epoch'] = data_factory.minibatches_per_epoch()\n",
    "params['source-vocab'] = data_factory.source_vocab_size\n",
    "params['target-vocab'] = data_factory.target_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Place holders for input and output data, first index is the minibatch size\n",
    "\n",
    "#For the input the second dimension will be passed a\n",
    "#     vector of size minibatch x (params['source-window']+params['target-window'])\n",
    "with tf.device('/cpu:0'):\n",
    "    input_indices = tf.placeholder(tf.int64, shape=[None, params['source-window']+params['target-window']])\n",
    "\n",
    "#The output will be given a minibatch vector of correct indicies\n",
    "with tf.device('/cpu:0'):\n",
    "    correct_output = tf.placeholder(tf.int64,shape=[None])\n",
    "\n",
    "#For passing in the learning rate and dropout rate\n",
    "with tf.device('/cpu:0'):\n",
    "    learning_rate = tf.placeholder(params['datatype'],shape=[])\n",
    "    dropout_rate = tf.placeholder(params['datatype'],shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def param_init(params,shape=None,name=None,datatype=None):\n",
    "    if datatype == None:\n",
    "        datatype = params['datatype']\n",
    "    assert shape != None,\"Error shape cannot be None in param_init\"\n",
    "    if params['init-method'] == 'uniform':\n",
    "        return tf.random_uniform(shape, minval=-1*params['init-range'],maxval=params['init-range'],\\\n",
    "                    dtype=datatype, seed=params['seed'], name=name)\n",
    "    else:\n",
    "        print \"ERROR this init-method has not been created yet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parameters for the source and target embeddings\n",
    "with tf.device('/cpu:0'):\n",
    "    source_emb_matrix = tf.Variable(param_init(params,shape=[params['source-vocab'],params['emb-size']],name='src-emb'))\n",
    "    target_emb_matrix = tf.Variable(param_init(params,shape=[params['target-vocab'],params['emb-size']],name='tgt-emb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Do the one-hot emebedding lookups\n",
    "src_embed = tf.nn.embedding_lookup(source_emb_matrix, tf.slice(input_indices,[0,0],[-1,params['source-window']]))\n",
    "tgt_embed = tf.nn.embedding_lookup(target_emb_matrix, tf.slice(input_indices,[0,params['source-window']],\\\n",
    "                                                                [-1,params['target-window']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now reshape to be able to feed through non-linearity\n",
    "concat_embed = tf.concat(1, [src_embed, tgt_embed])\n",
    "concat_embed = tf.reshape(concat_embed,[-1,params['emb-size']*(params['source-window']+params['target-window'])])\n",
    "concat_embed = tf.nn.dropout(concat_embed, 1-dropout_rate) #Pass in the keep prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First Layer\n",
    "with tf.device('/cpu:0'):\n",
    "    layer_1_weights = tf.Variable(param_init(params,\\\n",
    "        shape=[params['emb-size']*(params['source-window']+params['target-window']),params['hiddenstate-size']],name='lyr-1'))\n",
    "    layer_1_bias = tf.Variable(param_init(params,shape=[params['hiddenstate-size']],name='lyr-1-bias'))\n",
    "layer_1_output = tf.nn.relu(tf.matmul(concat_embed,layer_1_weights)+layer_1_bias)\n",
    "layer_1_output = tf.nn.dropout(layer_1_output, 1-dropout_rate) #Pass in the keep prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Second layer\n",
    "with tf.device('/cpu:0'):\n",
    "    layer_2_weights = tf.Variable(param_init(params,shape=[params['hiddenstate-size'],params['hiddenstate-size']],\\\n",
    "                            name='lyr-2'))\n",
    "    layer_2_bias = tf.Variable(param_init(params,shape=[params['hiddenstate-size']],name='lyr-2-bias'))\n",
    "layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,layer_2_weights)+layer_2_bias)\n",
    "layer_2_output = tf.nn.dropout(layer_2_output, 1-dropout_rate) #Pass in the keep prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Softmax layer\n",
    "with tf.device('/cpu:0'):\n",
    "    softmax_weights = tf.Variable(param_init(params,shape=[params['hiddenstate-size'],params['target-vocab']],\\\n",
    "                                  name='softmax-weights')) \n",
    "    softmax_bias = tf.Variable(param_init(params,shape=[params['target-vocab']],name='softmax-bias'))\n",
    "unscaled_final_output = tf.matmul(layer_2_output, softmax_weights) + softmax_bias\n",
    "loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(unscaled_final_output,correct_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initiailize all the variables\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Additional variables for tracking loss\n",
    "val_perplexities = [] #stores the validation perplexities each validation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Training for\",params['epochs'],\"epochs (\",params['epochs']*params['minibatches-per-epoch'],\"minibatches)\"\n",
    "print \"All current parameters:\"\n",
    "print params,'\\n\\n'\n",
    "print \"-\"*10,'beginning training','-'*10\n",
    "for i in range(params['epochs']*params['minibatches-per-epoch']):\n",
    "    #get the minibatch\n",
    "    curr_minibatch,eval_val = data_factory.get_minibatch()\n",
    "    if eval_val:\n",
    "        start_val = time.time()\n",
    "        print \"Getting validation perplexity\"\n",
    "        log_sum = 0\n",
    "        total_words = 0\n",
    "        for val_batch in data_factory.get_val_data_gen():\n",
    "            #print val_batch[:,:params['source-window']+params['target-window']].shape\n",
    "            #print np.squeeze(val_batch[:,params['source-window']+params['target-window']:]).shape\n",
    "            input_val_batch = val_batch[:,:params['source-window']+params['target-window']]\n",
    "            output_val_batch = np.squeeze(np.copy(val_batch[:,params['source-window']+params['target-window']:]))\n",
    "            log_sum+=loss.eval(feed_dict={input_indices:input_val_batch,\n",
    "                        correct_output:output_val_batch, \n",
    "                        dropout_rate:0.0})\n",
    "            total_words+=val_batch.shape[0]\n",
    "        log_sum = (log_sum/np.log(2.0))/total_words\n",
    "        print \"Perplexity on validation set:\",2**log_sum\n",
    "        val_perplexities.append(2**log_sum)\n",
    "        end_val = time.time()\n",
    "        print \"Time for perplexity on dev set (minutes):\",(end_val - start_val)/60.0\n",
    "    #Now update the gradients for this training batch\n",
    "    input_train_batch = curr_minibatch[:,:params['source-window']+params['target-window']]\n",
    "    output_train_batch = np.squeeze(np.copy(curr_minibatch[:,params['source-window']+params['target-window']:]))\n",
    "    train_op.run(feed_dict={input_indices:input_train_batch,\n",
    "                correct_output:output_train_batch,\\\n",
    "                dropout_rate:params['dropout-rate'],learning_rate:params['learning-rate'] })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
