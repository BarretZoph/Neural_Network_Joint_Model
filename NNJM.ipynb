{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NNJM\n",
    "#TODO\n",
    "# Check to be sure slices are being done correctly\n",
    "# Convolution instead of first dense hidden layer\n",
    "# Put in character CNN\n",
    "# Put in bi-dir LSTm\n",
    "# Add in different loss function\n",
    "# Add in argparse\n",
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import statements\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import codecs\n",
    "import types\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parameters for the model\n",
    "params = {}\n",
    "params['minibatch'] = 64\n",
    "params['input-emb-size'] = 100\n",
    "params['output-emb-size'] = 100\n",
    "params['hiddenstate-size'] = 400\n",
    "params['source-vocab'] = None\n",
    "params['target-vocab'] = None\n",
    "params['datatype'] = tf.float32\n",
    "params['init-method'] = 'uniform'\n",
    "params['init-range'] = 0.01\n",
    "params['source-window'] = 11\n",
    "params['target-window'] = 4\n",
    "params['use-char'] = False\n",
    "params['seed'] = 1\n",
    "params['dropout-rate'] = 0.0 #probability of dropping a node\n",
    "params['loss'] = 'MLE' #('MLE','NCE','IS')\n",
    "params['learning-rate'] = 0.1\n",
    "params['decrease-factor'] = 0.8 #Multiply the learning rate by this if dev perplexity increases by more than epsilon\n",
    "params['epsilon-criteria'] = 0 #Epsilon in decrease-factor\n",
    "params['val-check-rate'] = 0.5 #every fraction of the training set get val perplexity\n",
    "params['epochs'] = 1000 #how many epochs to train for\n",
    "params['minibatches-per-epoch'] = None #Filled in by the data preprep\n",
    "params['source-train-file'] = 'train.source.lc'\n",
    "params['target-train-file'] = 'train.target.lc'\n",
    "params['mapping-file-name'] = 'mapping.nn'\n",
    "params['count-cutoff'] = 3\n",
    "params['training-data-file-name'] = 'training.data.11+4.smaller'\n",
    "params['val-data-file-name'] = 'training.data.11+4.smaller'#'validation.data.11+4'\n",
    "params['use-gpu'] = False\n",
    "\n",
    "#Character model params\n",
    "char_params = {}\n",
    "char_params['num-highway-layers'] = 4\n",
    "char_params['char-emb-size'] = 25\n",
    "char_params['filter-width'] = 7\n",
    "char_params['longest-word'] = 30\n",
    "\n",
    "\n",
    "params['char-params'] = char_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Error checking for user inputs\n",
    "class NNJM_Error(Exception):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "try:\n",
    "    if not params['val-check-rate']>0: raise NNJM_Error('val-check-rate')\n",
    "    if not params['val-check-rate']<=1: raise NNJM_Error('val-check-rate')\n",
    "    if not params['dropout-rate']>=0: raise NNJM_Error('dropout-rate')\n",
    "    if not params['dropout-rate']<=1: raise NNJM_Error('dropout-rate')\n",
    "    if not params['init-range']>0: raise NNJM_Error('init-range')\n",
    "    if not params['minibatch']>0: raise NNJM_Error('minibatch')\n",
    "    if not params['learning-rate']>=0: raise NNJM_Error('learning-rate')\n",
    "    if not type(params['use-gpu'])==types.BooleanType: raise NNJM_Error('use-gpu')\n",
    "    if not type(params['use-char'])==types.BooleanType: raise NNJM_Error('use-char')\n",
    "    if not params['loss'] in ['MLE','NCE','IS']: raise NNJM_Error('loss')\n",
    "except NNJM_Error as e:\n",
    "        print 'Bad user input was entered for:', e.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Interactive session\n",
    "np.random.seed(seed=params['seed'])\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Process the data, get the source-vocab and target-vocab variables\n",
    "import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source file name used for creating mapping file: train.source.lc\n",
      "target file name used for creating mapping file: train.target.lc\n",
      "source vocab size: 46166\n",
      "target vocab size: 22701\n"
     ]
    }
   ],
   "source": [
    "#create the mapping file\n",
    "data_loader.create_word_mapping_file(params['source-train-file'],params['target-train-file'],\\\n",
    "            params['mapping-file-name'],params['count-cutoff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source mapping size: 46166\n",
      "Target mapping size: 22701\n",
      "src_window: 11\n",
      "tgt_window: 4\n",
      "minibatch_size: 64\n",
      "Number of training examples: 1000\n",
      "val stride: 32\n",
      "val shape: (1000, 16)\n",
      "Number of times per epoch the validation set will be evaluated: 2\n"
     ]
    }
   ],
   "source": [
    "#Now load in the data\n",
    "data_factory = data_loader.minibatcher(params['mapping-file-name'],params['training-data-file-name'],\\\n",
    "                params['val-data-file-name'],params['source-window'],params['target-window'],\\\n",
    "                params['minibatch'],params['val-check-rate'])\n",
    "params['minibatches-per-epoch'] = data_factory.minibatches_per_epoch()\n",
    "params['source-vocab'] = data_factory.source_vocab_size\n",
    "params['target-vocab'] = data_factory.target_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Place holders for input and output data, first index is the minibatch size\n",
    "\n",
    "#For the input the second dimension will be passed a\n",
    "#     vector of size minibatch x (params['source-window']+params['target-window'])\n",
    "with tf.device('/gpu:0' if params['use-gpu'] else '/cpu:0'):\n",
    "    input_indices = tf.placeholder(tf.int64, shape=[None, params['source-window']+params['target-window']])\n",
    "\n",
    "#The output will be given a minibatch vector of correct indicies\n",
    "with tf.device('/gpu:0' if params['use-gpu'] else '/cpu:0'):\n",
    "    correct_output = tf.placeholder(tf.int64,shape=[None])\n",
    "\n",
    "#For passing in the learning rate and dropout rate\n",
    "with tf.device('/gpu:0' if params['use-gpu'] else '/cpu:0'):\n",
    "    learning_rate = tf.placeholder(params['datatype'],shape=[])\n",
    "    dropout_rate = tf.placeholder(params['datatype'],shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def param_init(params,shape=None,name=None,datatype=None):\n",
    "    if datatype == None:\n",
    "        datatype = params['datatype']\n",
    "    assert shape != None,\"Error shape cannot be None in param_init\"\n",
    "    if params['init-method'] == 'uniform':\n",
    "        return tf.random_uniform(shape, minval=-1*params['init-range'],maxval=params['init-range'],\\\n",
    "                    dtype=datatype, seed=params['seed'], name=name)\n",
    "    else:\n",
    "        print \"ERROR this init-method has not been created yet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parameters for the source and target embeddings\n",
    "with tf.device('/gpu:0' if params['use-gpu'] else '/cpu:0'):\n",
    "    source_emb_matrix = tf.Variable(param_init(params,shape=[params['source-vocab'],params['input-emb-size']],name='src-emb'))\n",
    "    target_emb_matrix = tf.Variable(param_init(params,shape=[params['target-vocab'],params['input-emb-size']],name='tgt-emb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Do the one-hot emebedding lookups\n",
    "src_embed = tf.nn.embedding_lookup(source_emb_matrix, tf.slice(input_indices,[0,0],[-1,params['source-window']]))\n",
    "tgt_embed = tf.nn.embedding_lookup(target_emb_matrix, tf.slice(input_indices,[0,params['source-window']],\\\n",
    "                                                                [-1,params['target-window']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now reshape to be able to feed through non-linearity\n",
    "concat_embed = tf.concat(1, [src_embed, tgt_embed])\n",
    "concat_embed = tf.reshape(concat_embed,[-1,params['input-emb-size']*(params['source-window']+params['target-window'])])\n",
    "concat_embed = tf.nn.dropout(concat_embed, 1-dropout_rate) #Pass in the keep prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First Layer\n",
    "with tf.device('/gpu:0' if params['use-gpu'] else '/cpu:0'):\n",
    "    layer_1_weights = tf.Variable(param_init(params,\\\n",
    "        shape=[params['input-emb-size']*(params['source-window']+params['target-window']),params['hiddenstate-size']],name='lyr-1'))\n",
    "    layer_1_bias = tf.Variable(param_init(params,shape=[params['hiddenstate-size']],name='lyr-1-bias'))\n",
    "layer_1_output = tf.nn.relu(tf.matmul(concat_embed,layer_1_weights)+layer_1_bias)\n",
    "layer_1_output = tf.nn.dropout(layer_1_output, 1-dropout_rate) #Pass in the keep prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Second layer\n",
    "with tf.device('/gpu:0' if params['use-gpu'] else '/cpu:0'):\n",
    "    layer_2_weights = tf.Variable(param_init(params,shape=[params['hiddenstate-size'],params['output-emb-size']],\\\n",
    "                            name='lyr-2'))\n",
    "    layer_2_bias = tf.Variable(param_init(params,shape=[params['output-emb-size']],name='lyr-2-bias'))\n",
    "layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,layer_2_weights)+layer_2_bias)\n",
    "layer_2_output = tf.nn.dropout(layer_2_output, 1-dropout_rate) #Pass in the keep prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Softmax layer\n",
    "with tf.device('/gpu:0' if params['use-gpu'] else '/cpu:0'):\n",
    "    softmax_weights = tf.Variable(param_init(params,shape=[params['output-emb-size'],params['target-vocab']],\\\n",
    "                                  name='softmax-weights')) \n",
    "    softmax_bias = tf.Variable(param_init(params,shape=[params['target-vocab']],name='softmax-bias'))\n",
    "unscaled_final_output = tf.matmul(layer_2_output, softmax_weights) + softmax_bias\n",
    "loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(unscaled_final_output,correct_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initiailize all the variables\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Additional variables for tracking loss\n",
    "val_perplexities = [] #stores the validation perplexities each validation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 epochs ( 16000 minibatches)\n",
      "All current parameters:\n",
      "{'source-train-file': 'train.source.lc', 'char-params': {'filter-width': 7, 'longest-word': 30, 'char-emb-size': 25, 'num-highway-layers': 4}, 'input-emb-size': 100, 'val-check-rate': 0.5, 'epochs': 1000, 'use-char': False, 'seed': 1, 'learning-rate': 0.1, 'init-method': 'uniform', 'output-emb-size': 100, 'mapping-file-name': 'mapping.nn', 'hiddenstate-size': 400, 'dropout-rate': 0.0, 'target-window': 4, 'count-cutoff': 3, 'source-window': 11, 'use-gpu': False, 'val-data-file-name': 'training.data.11+4.smaller', 'init-range': 0.01, 'loss': 'MLE', 'target-train-file': 'train.target.lc', 'source-vocab': 46166, 'epsilon-criteria': 0, 'datatype': tf.float32, 'target-vocab': 22701, 'minibatches-per-epoch': 16, 'minibatch': 64, 'decrease-factor': 0.8, 'training-data-file-name': 'training.data.11+4.smaller'} \n",
      "\n",
      "\n",
      "---------- beginning training ----------\n",
      "Perplexity on validation set: 22702.6361827\n",
      "Time for perplexity on dev set (minutes): 0.00718978643417\n",
      "Perplexity on validation set: 5783.24627758\n",
      "Time for perplexity on dev set (minutes): 0.00671654939651\n",
      "---------- Epoch 1 just finished ----------\n",
      "Epoch time (minutes) 0.0291063825289\n",
      "Perplexity on validation set: 6746.66943457\n",
      "Decreased learning rate to: 0.08\n",
      "Time for perplexity on dev set (minutes): 0.00697721640269\n",
      "Perplexity on validation set: 4574.08122686\n",
      "Time for perplexity on dev set (minutes): 0.00663801431656\n",
      "---------- Epoch 2 just finished ----------\n",
      "Epoch time (minutes) 0.0294322808584\n",
      "Perplexity on validation set: 2608.94553313\n",
      "Time for perplexity on dev set (minutes): 0.00658626556396\n",
      "Perplexity on validation set: 1960.34872052\n",
      "Time for perplexity on dev set (minutes): 0.00720993280411\n",
      "---------- Epoch 3 just finished ----------\n",
      "Epoch time (minutes) 0.0281027992566\n",
      "Perplexity on validation set: 2972.0538645\n",
      "Decreased learning rate to: 0.064\n",
      "Time for perplexity on dev set (minutes): 0.00677749713262\n",
      "Perplexity on validation set: 2405.46830635\n",
      "Time for perplexity on dev set (minutes): 0.00710628430049\n",
      "---------- Epoch 4 just finished ----------\n",
      "Epoch time (minutes) 0.0289463321368\n",
      "Perplexity on validation set: 2083.50734416\n",
      "Time for perplexity on dev set (minutes): 0.00743221839269\n",
      "Perplexity on validation set: 1857.94548408\n",
      "Time for perplexity on dev set (minutes): 0.00656716823578\n",
      "---------- Epoch 5 just finished ----------\n",
      "Epoch time (minutes) 0.0302493969599\n",
      "Perplexity on validation set: 1705.62154849\n",
      "Time for perplexity on dev set (minutes): 0.00664001703262\n",
      "Perplexity on validation set: 1600.80018026\n",
      "Time for perplexity on dev set (minutes): 0.006219013532\n",
      "---------- Epoch 6 just finished ----------\n",
      "Epoch time (minutes) 0.0283823331197\n",
      "Perplexity on validation set: 1506.06929942\n",
      "Time for perplexity on dev set (minutes): 0.00803455114365\n",
      "Perplexity on validation set: 1410.03611686\n",
      "Time for perplexity on dev set (minutes): 0.00659708579381\n",
      "---------- Epoch 7 just finished ----------\n",
      "Epoch time (minutes) 0.0331385652224\n",
      "Perplexity on validation set: 1333.23334596\n",
      "Time for perplexity on dev set (minutes): 0.00709678332011\n",
      "Perplexity on validation set: 1261.87005103\n",
      "Time for perplexity on dev set (minutes): 0.0080872853597\n",
      "---------- Epoch 8 just finished ----------\n",
      "Epoch time (minutes) 0.0341332157453\n",
      "Perplexity on validation set: 1147.47235341\n",
      "Time for perplexity on dev set (minutes): 0.00743003288905\n",
      "---------- Epoch 9 just finished ----------\n",
      "Epoch time (minutes) 0.023478547732\n",
      "Perplexity on validation set: 1098.69593508\n",
      "Time for perplexity on dev set (minutes): 0.00762315193812\n",
      "Perplexity on validation set: 1050.54040512\n",
      "Time for perplexity on dev set (minutes): 0.00697995026906\n",
      "---------- Epoch 10 just finished ----------\n",
      "Epoch time (minutes) 0.0328529993693\n",
      "Perplexity on validation set: 1009.10098705\n",
      "Time for perplexity on dev set (minutes): 0.00777555306753\n",
      "Perplexity on validation set: 970.775345548\n",
      "Time for perplexity on dev set (minutes): 0.0066331187884\n",
      "---------- Epoch 11 just finished ----------\n",
      "Epoch time (minutes) 0.0305501182874\n",
      "Perplexity on validation set: 943.219819632\n",
      "Time for perplexity on dev set (minutes): 0.00792630116145\n",
      "Perplexity on validation set: 909.793736622\n",
      "Time for perplexity on dev set (minutes): 0.00729094743729\n",
      "---------- Epoch 12 just finished ----------\n",
      "Epoch time (minutes) 0.0335129499435\n",
      "Perplexity on validation set: 881.174842487\n",
      "Time for perplexity on dev set (minutes): 0.00752991437912\n",
      "Perplexity on validation set: 854.703811502\n",
      "Time for perplexity on dev set (minutes): 0.00694179932276\n",
      "---------- Epoch 13 just finished ----------\n",
      "Epoch time (minutes) 0.0317640980085\n",
      "Perplexity on validation set: 829.587392044\n",
      "Time for perplexity on dev set (minutes): 0.00740559895833\n",
      "Perplexity on validation set: 809.951088839\n",
      "Time for perplexity on dev set (minutes): 0.00737989743551\n",
      "---------- Epoch 14 just finished ----------\n",
      "Epoch time (minutes) 0.0318072517713\n",
      "Perplexity on validation set: 789.322121715\n",
      "Time for perplexity on dev set (minutes): 0.00728555123011\n",
      "Perplexity on validation set: 767.680889148\n",
      "Time for perplexity on dev set (minutes): 0.00763791799545\n",
      "---------- Epoch 15 just finished ----------\n",
      "Epoch time (minutes) 0.0323158184687\n",
      "Perplexity on validation set: 748.06341577\n",
      "Time for perplexity on dev set (minutes): 0.00744766791662\n",
      "Perplexity on validation set: 730.542936135\n",
      "Time for perplexity on dev set (minutes): 0.00696746905645\n",
      "---------- Epoch 16 just finished ----------\n",
      "Epoch time (minutes) 0.0323526501656\n",
      "Perplexity on validation set: 699.843764597\n",
      "Time for perplexity on dev set (minutes): 0.00717435280482\n",
      "---------- Epoch 17 just finished ----------\n",
      "Epoch time (minutes) 0.0238883852959\n",
      "Perplexity on validation set: 684.247526266\n",
      "Time for perplexity on dev set (minutes): 0.00737684965134\n",
      "Perplexity on validation set: 669.945724604\n",
      "Time for perplexity on dev set (minutes): 0.00753401915232\n",
      "---------- Epoch 18 just finished ----------\n",
      "Epoch time (minutes) 0.0334758003553\n",
      "Perplexity on validation set: 655.074605221\n",
      "Time for perplexity on dev set (minutes): 0.00885999997457\n",
      "Perplexity on validation set: 642.052819833\n",
      "Time for perplexity on dev set (minutes): 0.00746510028839\n",
      "---------- Epoch 19 just finished ----------\n",
      "Epoch time (minutes) 0.0337487022082\n",
      "Perplexity on validation set: 631.665947971\n",
      "Time for perplexity on dev set (minutes): 0.00813413461049\n",
      "Perplexity on validation set: 619.245197045\n",
      "Time for perplexity on dev set (minutes): 0.00762843290965\n",
      "---------- Epoch 20 just finished ----------\n",
      "Epoch time (minutes) 0.0336612025897\n",
      "Perplexity on validation set: 606.966320744\n",
      "Time for perplexity on dev set (minutes): 0.00872824986776\n",
      "Perplexity on validation set: 596.462909464\n",
      "Time for perplexity on dev set (minutes): 0.00759315093358\n",
      "---------- Epoch 21 just finished ----------\n",
      "Epoch time (minutes) 0.0356690009435\n",
      "Perplexity on validation set: 584.844888628\n",
      "Time for perplexity on dev set (minutes): 0.0092974503835\n",
      "Perplexity on validation set: 576.870216646\n",
      "Time for perplexity on dev set (minutes): 0.008642466863\n",
      "---------- Epoch 22 just finished ----------\n",
      "Epoch time (minutes) 0.0363831520081\n",
      "Perplexity on validation set: 566.848071443\n",
      "Time for perplexity on dev set (minutes): 0.00787951946259\n",
      "Perplexity on validation set: 556.876104196\n",
      "Time for perplexity on dev set (minutes): 0.00710411866506\n",
      "---------- Epoch 23 just finished ----------\n",
      "Epoch time (minutes) 0.0326615492503\n",
      "Perplexity on validation set: 546.813597085\n",
      "Time for perplexity on dev set (minutes): 0.00747829675674\n",
      "Perplexity on validation set: 538.476160326\n",
      "Time for perplexity on dev set (minutes): 0.00707105000814\n",
      "---------- Epoch 24 just finished ----------\n",
      "Epoch time (minutes) 0.0339673161507\n",
      "Perplexity on validation set: 523.001709162\n",
      "Time for perplexity on dev set (minutes): 0.00752588113149\n",
      "---------- Epoch 25 just finished ----------\n",
      "Epoch time (minutes) 0.0244701147079\n",
      "Perplexity on validation set: 514.606843399\n",
      "Time for perplexity on dev set (minutes): 0.00734634796778\n",
      "Perplexity on validation set: 507.200114161\n",
      "Time for perplexity on dev set (minutes): 0.00693419774373\n",
      "---------- Epoch 26 just finished ----------\n",
      "Epoch time (minutes) 0.0318279504776\n",
      "Perplexity on validation set: 499.02337525\n",
      "Time for perplexity on dev set (minutes): 0.00858770211538\n",
      "Perplexity on validation set: 491.968709236\n",
      "Time for perplexity on dev set (minutes): 0.0084318002065\n",
      "---------- Epoch 27 just finished ----------\n",
      "Epoch time (minutes) 0.0329554835955\n",
      "Perplexity on validation set: 486.410199354\n",
      "Time for perplexity on dev set (minutes): 0.00849711894989\n",
      "Perplexity on validation set: 479.469890558\n",
      "Time for perplexity on dev set (minutes): 0.00684709946314\n",
      "---------- Epoch 28 just finished ----------\n",
      "Epoch time (minutes) 0.0320447166761\n",
      "Perplexity on validation set: 472.601991273\n",
      "Time for perplexity on dev set (minutes): 0.00716843207677\n",
      "Perplexity on validation set: 466.674627665\n",
      "Time for perplexity on dev set (minutes): 0.00802411635717\n",
      "---------- Epoch 29 just finished ----------\n",
      "Epoch time (minutes) 0.0331876834234\n",
      "Perplexity on validation set: 459.97460956\n",
      "Time for perplexity on dev set (minutes): 0.00850503444672\n",
      "Perplexity on validation set: 455.533552869\n",
      "Time for perplexity on dev set (minutes): 0.00719736417135\n",
      "---------- Epoch 30 just finished ----------\n",
      "Epoch time (minutes) 0.0324935158094\n",
      "Perplexity on validation set: 449.787193083\n",
      "Time for perplexity on dev set (minutes): 0.00774073203405\n",
      "Perplexity on validation set: 443.802397905\n",
      "Time for perplexity on dev set (minutes): 0.0090155005455\n",
      "---------- Epoch 31 just finished ----------\n",
      "Epoch time (minutes) 0.0339174509048\n",
      "Perplexity on validation set: 437.880355912\n",
      "Time for perplexity on dev set (minutes): 0.00698351462682\n",
      "Perplexity on validation set: 432.910977476\n",
      "Time for perplexity on dev set (minutes): 0.00734170277913\n",
      "---------- Epoch 32 just finished ----------\n",
      "Epoch time (minutes) 0.034123301506\n"
     ]
    }
   ],
   "source": [
    "print \"Training for\",params['epochs'],\"epochs (\",params['epochs']*params['minibatches-per-epoch'],\"minibatches)\"\n",
    "print \"All current parameters:\"\n",
    "print params,'\\n\\n'\n",
    "print \"-\"*10,'beginning training','-'*10\n",
    "start_train = time.time()\n",
    "data_factory.prep_train() #prep the data loader for training, also stores timing info\n",
    "for i in range(params['epochs']*params['minibatches-per-epoch']):\n",
    "    #get the minibatch\n",
    "    curr_minibatch,eval_val = data_factory.get_minibatch()\n",
    "    if eval_val:\n",
    "        start_val = time.time()\n",
    "        log_sum = 0\n",
    "        total_words = 0\n",
    "        for val_batch in data_factory.get_val_data_gen():\n",
    "            #print val_batch[:,:params['source-window']+params['target-window']].shape\n",
    "            #print np.squeeze(val_batch[:,params['source-window']+params['target-window']:]).shape\n",
    "            input_val_batch = val_batch[:,:params['source-window']+params['target-window']]\n",
    "            output_val_batch = np.squeeze(np.copy(val_batch[:,params['source-window']+params['target-window']:]))\n",
    "            log_sum+=loss.eval(feed_dict={input_indices:input_val_batch,\n",
    "                        correct_output:output_val_batch, \n",
    "                        dropout_rate:0.0})\n",
    "            total_words+=val_batch.shape[0]\n",
    "        log_sum = (log_sum/np.log(2.0))/total_words\n",
    "        print \"Perplexity on validation set:\",2**log_sum\n",
    "        val_perplexities.append(2**log_sum)\n",
    "        if (len(val_perplexities) > 1) and (val_perplexities[-2] + params['epsilon-criteria'] < val_perplexities[-1]):\n",
    "            params['learning-rate']*=params['decrease-factor']\n",
    "            print 'Decreased learning rate to:',params['learning-rate']\n",
    "        end_val = time.time()\n",
    "        print \"Time for perplexity on dev set (minutes):\",(end_val - start_val)/60.0\n",
    "    #Now update the gradients for this training batch\n",
    "    input_train_batch = curr_minibatch[:,:params['source-window']+params['target-window']]\n",
    "    output_train_batch = np.squeeze(np.copy(curr_minibatch[:,params['source-window']+params['target-window']:]))\n",
    "    train_op.run(feed_dict={input_indices:input_train_batch,\n",
    "                correct_output:output_train_batch,\\\n",
    "                dropout_rate:params['dropout-rate'],learning_rate:params['learning-rate'] })\n",
    "\n",
    "end_train = time.time()\n",
    "print \"Time for total training (minutes):\",(end_train - start_train)/60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
